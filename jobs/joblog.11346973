==========================================
Job started on g14 at Sat Nov  8 22:25:58 PST 2025
Job ID: 11346973
Working directory: /u/scratch/r/rgorzek/foundation-cross
==========================================

Environment Information:
----------------------------------------
Python version:
Python 3.11.13

PyTorch version:
2.1.2+cu121

CUDA available:
True
GPU device:
NVIDIA RTX A6000
----------------------------------------

Configuration file: configs/experiments/mouse_to_opossum_scgpt.yaml

Starting experiment...
==========================================
Starting experiment from config: configs/experiments/mouse_to_opossum_scgpt.yaml
2025-11-08 22:26:10 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:26:10 - pipeline - INFO - Cross-Species Label Transfer Pipeline
2025-11-08 22:26:10 - pipeline - INFO - =========================================================================================
2025-11-08 22:26:10 - pipeline - INFO - Experiment: mouse_opossum_transfer
2025-11-08 22:26:10 - pipeline - INFO - Output directory: results/mouse_opossum/scgpt_Nov08-22-26
2025-11-08 22:26:10 - pipeline - INFO - Configuration saved to results/mouse_opossum/scgpt_Nov08-22-26/config.yaml
2025-11-08 22:26:10 - pipeline - INFO - Git commit: f4785bae3eccec4aa2ac578c79354f452252e1ac
2025-11-08 22:26:10 - pipeline - WARNING - Git repository has uncommitted changes
2025-11-08 22:26:10 - pipeline - INFO - Python version: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]
2025-11-08 22:26:10 - pipeline - INFO - PyTorch version: 2.1.2+cu121
2025-11-08 22:26:10 - pipeline - INFO - CUDA available: True
2025-11-08 22:26:10 - pipeline - INFO - Config hash: 73a2db97
2025-11-08 22:26:10 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:26:10 - pipeline - INFO - STEP 1: Loading Datasets
2025-11-08 22:26:10 - pipeline - INFO - =========================================================================================
2025-11-08 22:26:10 - pipeline - INFO - Loading mouse data from data/raw/Mouse_V1_P38_All.h5ad
2025-11-08 22:27:08 - pipeline - INFO -   16997 cells, 24372 genes
2025-11-08 22:27:09 - pipeline - WARNING - Reference has 57 genes with zero expression
2025-11-08 22:27:09 - pipeline - INFO - Loading opossum data from data/raw/Opossum_V1_All.h5ad
2025-11-08 22:27:21 - pipeline - INFO -   32764 cells, 30800 genes
2025-11-08 22:27:21 - pipeline - WARNING - Query 0 has 1412 genes with zero expression
2025-11-08 22:27:21 - pipeline - INFO - 
Finding common genes across datasets...
2025-11-08 22:27:21 - pipeline - INFO - Found 11730 common genes across all datasets
2025-11-08 22:27:27 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:27:27 - pipeline - INFO - STEP 2: Preprocessing Datasets
2025-11-08 22:27:27 - pipeline - INFO - =========================================================================================
2025-11-08 22:27:27 - pipeline - INFO - Filtering reference data...
2025-11-08 22:27:31 - pipeline - INFO - 
Filtering query data: opossum...
2025-11-08 22:27:36 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:27:36 - pipeline - INFO - STEP 3: Preparing Data for Training
2025-11-08 22:27:36 - pipeline - INFO - =========================================================================================
2025-11-08 22:27:36 - pipeline - INFO - Label matching summary:
2025-11-08 22:27:36 - pipeline - INFO -   18305 query cells have labels matching reference categories
2025-11-08 22:27:36 - pipeline - INFO -   14459 query cells have labels NOT in reference
2025-11-08 22:27:36 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:27:36 - pipeline - INFO - STEP 4: Initializing Model
2025-11-08 22:27:36 - pipeline - INFO - =========================================================================================
/u/home/r/rgorzek/.conda/envs/scgpt_env/lib/python3.11/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
/u/home/r/rgorzek/.conda/envs/scgpt_env/lib/python3.11/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed
  warnings.warn("flash_attn is not installed")
2025-11-08 22:27:38 - pipeline - INFO - Initialized scgpt model
2025-11-08 22:27:38 - pipeline - INFO -   Output directory: results/mouse_opossum/scgpt_Nov08-22-26
2025-11-08 22:27:38 - pipeline - INFO - Using device: cuda
2025-11-08 22:27:38 - pipeline - INFO - 
=========================================================================================
2025-11-08 22:27:38 - pipeline - INFO - STEP 5: Training Model
2025-11-08 22:27:38 - pipeline - INFO - =========================================================================================
2025-11-08 22:27:38 - pipeline - INFO - Random seed: 0
2025-11-08 22:27:38 - pipeline - INFO - Training scGPT model
2025-11-08 22:27:38 - pipeline - INFO - Loaded pre-trained vocabulary from models/scgpt/whole-human/vocab.json
2025-11-08 22:27:38 - pipeline - INFO - Converting gene names to uppercase to match vocabulary
2025-11-08 22:27:38 - pipeline - INFO - Matched 11204/11730 genes in vocabulary of size 60697
scGPT - INFO - Filtering cells by counts ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Binning data ...
2025-11-08 22:27:54 - pipeline - INFO - Training samples: 15297
2025-11-08 22:27:54 - pipeline - INFO - Validation samples: 1700
2025-11-08 22:28:18 - pipeline - INFO - Loaded pretrained architecture: embsize=512, nhead=8, d_hid=512, nlayers=12
2025-11-08 22:28:18 - pipeline - INFO - Loaded architecture from pretrained args.json
/u/home/r/rgorzek/.conda/envs/scgpt_env/lib/python3.11/site-packages/scgpt/model/model.py:77: UserWarning: flash-attn is not installed, using pytorch transformer instead. Set use_fast_transformer=False to avoid this warning. Installing flash-attn is highly recommended.
  warnings.warn(
2025-11-08 22:28:18 - pipeline - WARNING - Failed to load all weights. Loading compatible weights only.
2025-11-08 22:28:18 - pipeline - INFO - Loaded 143 compatible weight tensors
2025-11-08 22:28:18 - pipeline - INFO - Trainable parameters: 51,341,845 / 51,341,845
2025-11-08 22:29:51 - pipeline - INFO - | epoch   1 | 100/957 batches | lr 0.0001 | ms/batch 920.33 | loss 2.6730 | err 0.8391
2025-11-08 22:31:23 - pipeline - INFO - | epoch   1 | 200/957 batches | lr 0.0001 | ms/batch 921.70 | loss 2.5788 | err 0.8218
2025-11-08 22:32:55 - pipeline - INFO - | epoch   1 | 300/957 batches | lr 0.0001 | ms/batch 921.99 | loss 2.3819 | err 0.7766
2025-11-08 22:34:27 - pipeline - INFO - | epoch   1 | 400/957 batches | lr 0.0001 | ms/batch 920.25 | loss 2.2004 | err 0.7350
2025-11-08 22:35:59 - pipeline - INFO - | epoch   1 | 500/957 batches | lr 0.0001 | ms/batch 919.42 | loss 2.0532 | err 0.7011
2025-11-08 22:37:31 - pipeline - INFO - | epoch   1 | 600/957 batches | lr 0.0001 | ms/batch 918.51 | loss 1.9080 | err 0.6604
2025-11-08 22:39:03 - pipeline - INFO - | epoch   1 | 700/957 batches | lr 0.0001 | ms/batch 918.45 | loss 1.7760 | err 0.6172
2025-11-08 22:40:35 - pipeline - INFO - | epoch   1 | 800/957 batches | lr 0.0001 | ms/batch 917.97 | loss 1.6362 | err 0.5677
2025-11-08 22:42:06 - pipeline - INFO - | epoch   1 | 900/957 batches | lr 0.0001 | ms/batch 917.94 | loss 1.5165 | err 0.5243
2025-11-08 22:43:26 - pipeline - INFO - Epoch   1 | train loss 1.4531 | valid loss 0.3822 | valid error 0.1176
2025-11-08 22:43:26 - pipeline - INFO - New best model at epoch 1
2025-11-08 22:45:00 - pipeline - INFO - | epoch   2 | 100/957 batches | lr 0.0001 | ms/batch 936.69 | loss 0.3928 | err 0.1244
2025-11-08 22:46:32 - pipeline - INFO - | epoch   2 | 200/957 batches | lr 0.0001 | ms/batch 917.54 | loss 0.3843 | err 0.1197
2025-11-08 22:48:04 - pipeline - INFO - | epoch   2 | 300/957 batches | lr 0.0001 | ms/batch 916.96 | loss 0.3519 | err 0.1107
2025-11-08 22:49:35 - pipeline - INFO - | epoch   2 | 400/957 batches | lr 0.0001 | ms/batch 917.16 | loss 0.3263 | err 0.1021
2025-11-08 22:51:07 - pipeline - INFO - | epoch   2 | 500/957 batches | lr 0.0001 | ms/batch 916.40 | loss 0.3197 | err 0.0983
2025-11-08 22:52:39 - pipeline - INFO - | epoch   2 | 600/957 batches | lr 0.0001 | ms/batch 916.81 | loss 0.3031 | err 0.0935
2025-11-08 22:54:10 - pipeline - INFO - | epoch   2 | 700/957 batches | lr 0.0001 | ms/batch 916.09 | loss 0.2898 | err 0.0887
2025-11-08 22:55:42 - pipeline - INFO - | epoch   2 | 800/957 batches | lr 0.0001 | ms/batch 916.26 | loss 0.2860 | err 0.0875
2025-11-08 22:57:13 - pipeline - INFO - | epoch   2 | 900/957 batches | lr 0.0001 | ms/batch 915.88 | loss 0.2796 | err 0.0850
2025-11-08 22:58:33 - pipeline - INFO - Epoch   2 | train loss 0.2763 | valid loss 0.1581 | valid error 0.0459
2025-11-08 22:58:33 - pipeline - INFO - New best model at epoch 2
2025-11-08 23:00:07 - pipeline - INFO - | epoch   3 | 100/957 batches | lr 0.0001 | ms/batch 935.91 | loss 0.2122 | err 0.0594
2025-11-08 23:01:39 - pipeline - INFO - | epoch   3 | 200/957 batches | lr 0.0001 | ms/batch 916.41 | loss 0.1875 | err 0.0553
2025-11-08 23:03:10 - pipeline - INFO - | epoch   3 | 300/957 batches | lr 0.0001 | ms/batch 915.76 | loss 0.1789 | err 0.0513
2025-11-08 23:04:42 - pipeline - INFO - | epoch   3 | 400/957 batches | lr 0.0001 | ms/batch 915.91 | loss 0.1786 | err 0.0493
2025-11-08 23:06:13 - pipeline - INFO - | epoch   3 | 500/957 batches | lr 0.0001 | ms/batch 915.66 | loss 0.1743 | err 0.0480
2025-11-08 23:07:45 - pipeline - INFO - | epoch   3 | 600/957 batches | lr 0.0001 | ms/batch 915.77 | loss 0.1743 | err 0.0483
2025-11-08 23:09:16 - pipeline - INFO - | epoch   3 | 700/957 batches | lr 0.0001 | ms/batch 915.90 | loss 0.1742 | err 0.0478
2025-11-08 23:10:48 - pipeline - INFO - | epoch   3 | 800/957 batches | lr 0.0001 | ms/batch 915.71 | loss 0.1728 | err 0.0473
2025-11-08 23:12:20 - pipeline - INFO - | epoch   3 | 900/957 batches | lr 0.0001 | ms/batch 915.60 | loss 0.1703 | err 0.0465
2025-11-08 23:13:39 - pipeline - INFO - Epoch   3 | train loss 0.1718 | valid loss 0.2430 | valid error 0.0712
2025-11-08 23:15:13 - pipeline - INFO - | epoch   4 | 100/957 batches | lr 0.0001 | ms/batch 935.47 | loss 0.1451 | err 0.0408
2025-11-08 23:16:45 - pipeline - INFO - | epoch   4 | 200/957 batches | lr 0.0001 | ms/batch 916.55 | loss 0.1170 | err 0.0336
2025-11-08 23:18:16 - pipeline - INFO - | epoch   4 | 300/957 batches | lr 0.0001 | ms/batch 915.63 | loss 0.1189 | err 0.0341
2025-11-08 23:19:48 - pipeline - INFO - | epoch   4 | 400/957 batches | lr 0.0001 | ms/batch 916.16 | loss 0.1262 | err 0.0352
2025-11-08 23:21:19 - pipeline - INFO - | epoch   4 | 500/957 batches | lr 0.0001 | ms/batch 916.05 | loss 0.1240 | err 0.0339
2025-11-08 23:22:51 - pipeline - INFO - | epoch   4 | 600/957 batches | lr 0.0001 | ms/batch 916.43 | loss 0.1284 | err 0.0348
2025-11-08 23:24:23 - pipeline - INFO - | epoch   4 | 700/957 batches | lr 0.0001 | ms/batch 916.30 | loss 0.1294 | err 0.0355
2025-11-08 23:25:54 - pipeline - INFO - | epoch   4 | 800/957 batches | lr 0.0001 | ms/batch 915.76 | loss 0.1286 | err 0.0350
2025-11-08 23:27:26 - pipeline - INFO - | epoch   4 | 900/957 batches | lr 0.0001 | ms/batch 916.22 | loss 0.1245 | err 0.0340
2025-11-08 23:28:46 - pipeline - INFO - Epoch   4 | train loss 0.1240 | valid loss 0.0892 | valid error 0.0241
2025-11-08 23:28:46 - pipeline - INFO - New best model at epoch 4
2025-11-08 23:30:19 - pipeline - INFO - | epoch   5 | 100/957 batches | lr 0.0001 | ms/batch 935.37 | loss 0.0956 | err 0.0272
2025-11-08 23:31:51 - pipeline - INFO - | epoch   5 | 200/957 batches | lr 0.0001 | ms/batch 916.83 | loss 0.1030 | err 0.0280
2025-11-08 23:33:23 - pipeline - INFO - | epoch   5 | 300/957 batches | lr 0.0001 | ms/batch 916.26 | loss 0.1083 | err 0.0291
2025-11-08 23:34:54 - pipeline - INFO - | epoch   5 | 400/957 batches | lr 0.0001 | ms/batch 916.00 | loss 0.0973 | err 0.0256
2025-11-08 23:36:26 - pipeline - INFO - | epoch   5 | 500/957 batches | lr 0.0001 | ms/batch 916.42 | loss 0.0993 | err 0.0254
2025-11-08 23:37:58 - pipeline - INFO - | epoch   5 | 600/957 batches | lr 0.0001 | ms/batch 916.43 | loss 0.0988 | err 0.0255
2025-11-08 23:39:29 - pipeline - INFO - | epoch   5 | 700/957 batches | lr 0.0001 | ms/batch 916.57 | loss 0.0992 | err 0.0257
2025-11-08 23:41:01 - pipeline - INFO - | epoch   5 | 800/957 batches | lr 0.0001 | ms/batch 916.68 | loss 0.0985 | err 0.0254
2025-11-08 23:42:33 - pipeline - INFO - | epoch   5 | 900/957 batches | lr 0.0001 | ms/batch 916.57 | loss 0.0978 | err 0.0255
2025-11-08 23:43:52 - pipeline - INFO - Epoch   5 | train loss 0.0963 | valid loss 0.1435 | valid error 0.0282
2025-11-08 23:45:26 - pipeline - INFO - | epoch   6 | 100/957 batches | lr 0.0001 | ms/batch 935.39 | loss 0.0942 | err 0.0235
2025-11-08 23:46:58 - pipeline - INFO - | epoch   6 | 200/957 batches | lr 0.0001 | ms/batch 916.14 | loss 0.0791 | err 0.0218
2025-11-08 23:48:29 - pipeline - INFO - | epoch   6 | 300/957 batches | lr 0.0001 | ms/batch 915.59 | loss 0.0843 | err 0.0224
2025-11-08 23:50:01 - pipeline - INFO - | epoch   6 | 400/957 batches | lr 0.0001 | ms/batch 915.56 | loss 0.0850 | err 0.0228
2025-11-08 23:51:32 - pipeline - INFO - | epoch   6 | 500/957 batches | lr 0.0001 | ms/batch 915.53 | loss 0.0886 | err 0.0222
2025-11-08 23:53:04 - pipeline - INFO - | epoch   6 | 600/957 batches | lr 0.0001 | ms/batch 915.58 | loss 0.0903 | err 0.0220
2025-11-08 23:54:35 - pipeline - INFO - | epoch   6 | 700/957 batches | lr 0.0001 | ms/batch 915.35 | loss 0.0916 | err 0.0222
2025-11-08 23:56:07 - pipeline - INFO - | epoch   6 | 800/957 batches | lr 0.0001 | ms/batch 915.60 | loss 0.0879 | err 0.0216
2025-11-08 23:57:39 - pipeline - INFO - | epoch   6 | 900/957 batches | lr 0.0001 | ms/batch 915.54 | loss 0.0908 | err 0.0222
2025-11-08 23:58:58 - pipeline - INFO - Epoch   6 | train loss 0.0905 | valid loss 0.0840 | valid error 0.0200
2025-11-08 23:58:58 - pipeline - INFO - New best model at epoch 6
2025-11-09 00:00:32 - pipeline - INFO - | epoch   7 | 100/957 batches | lr 0.0001 | ms/batch 935.00 | loss 0.0461 | err 0.0124
2025-11-09 00:02:03 - pipeline - INFO - | epoch   7 | 200/957 batches | lr 0.0001 | ms/batch 915.48 | loss 0.0556 | err 0.0143
2025-11-09 00:03:35 - pipeline - INFO - | epoch   7 | 300/957 batches | lr 0.0001 | ms/batch 915.17 | loss 0.0592 | err 0.0156
2025-11-09 00:05:06 - pipeline - INFO - | epoch   7 | 400/957 batches | lr 0.0001 | ms/batch 915.01 | loss 0.0630 | err 0.0161
2025-11-09 00:06:38 - pipeline - INFO - | epoch   7 | 500/957 batches | lr 0.0001 | ms/batch 915.26 | loss 0.0680 | err 0.0171
2025-11-09 00:08:10 - pipeline - INFO - | epoch   7 | 600/957 batches | lr 0.0001 | ms/batch 915.42 | loss 0.0674 | err 0.0168
2025-11-09 00:09:41 - pipeline - INFO - | epoch   7 | 700/957 batches | lr 0.0001 | ms/batch 915.22 | loss 0.0701 | err 0.0169
2025-11-09 00:11:13 - pipeline - INFO - | epoch   7 | 800/957 batches | lr 0.0001 | ms/batch 915.15 | loss 0.0702 | err 0.0169
2025-11-09 00:12:44 - pipeline - INFO - | epoch   7 | 900/957 batches | lr 0.0001 | ms/batch 915.16 | loss 0.0731 | err 0.0178
2025-11-09 00:14:04 - pipeline - INFO - Epoch   7 | train loss 0.0744 | valid loss 0.0911 | valid error 0.0235
2025-11-09 00:15:37 - pipeline - INFO - | epoch   8 | 100/957 batches | lr 0.0000 | ms/batch 934.77 | loss 0.0705 | err 0.0167
2025-11-09 00:17:09 - pipeline - INFO - | epoch   8 | 200/957 batches | lr 0.0000 | ms/batch 915.45 | loss 0.0644 | err 0.0146
2025-11-09 00:18:40 - pipeline - INFO - | epoch   8 | 300/957 batches | lr 0.0000 | ms/batch 915.18 | loss 0.0576 | err 0.0133
2025-11-09 00:20:12 - pipeline - INFO - | epoch   8 | 400/957 batches | lr 0.0000 | ms/batch 915.02 | loss 0.0583 | err 0.0143
2025-11-09 00:21:43 - pipeline - INFO - | epoch   8 | 500/957 batches | lr 0.0000 | ms/batch 915.01 | loss 0.0556 | err 0.0140
2025-11-09 00:23:15 - pipeline - INFO - | epoch   8 | 600/957 batches | lr 0.0000 | ms/batch 914.81 | loss 0.0559 | err 0.0142
2025-11-09 00:24:46 - pipeline - INFO - | epoch   8 | 700/957 batches | lr 0.0000 | ms/batch 914.99 | loss 0.0566 | err 0.0148
2025-11-09 00:26:18 - pipeline - INFO - | epoch   8 | 800/957 batches | lr 0.0000 | ms/batch 914.85 | loss 0.0547 | err 0.0147
2025-11-09 00:27:49 - pipeline - INFO - | epoch   8 | 900/957 batches | lr 0.0000 | ms/batch 914.38 | loss 0.0560 | err 0.0146
2025-11-09 00:29:09 - pipeline - INFO - Epoch   8 | train loss 0.0569 | valid loss 0.0873 | valid error 0.0188
2025-11-09 00:30:43 - pipeline - INFO - | epoch   9 | 100/957 batches | lr 0.0000 | ms/batch 934.16 | loss 0.0195 | err 0.0037
2025-11-09 00:32:14 - pipeline - INFO - | epoch   9 | 200/957 batches | lr 0.0000 | ms/batch 915.16 | loss 0.0317 | err 0.0068
2025-11-09 00:33:46 - pipeline - INFO - | epoch   9 | 300/957 batches | lr 0.0000 | ms/batch 915.27 | loss 0.0487 | err 0.0104
2025-11-09 00:35:17 - pipeline - INFO - | epoch   9 | 400/957 batches | lr 0.0000 | ms/batch 915.33 | loss 0.0510 | err 0.0117
2025-11-09 00:36:49 - pipeline - INFO - | epoch   9 | 500/957 batches | lr 0.0000 | ms/batch 914.94 | loss 0.0489 | err 0.0116
2025-11-09 00:38:20 - pipeline - INFO - | epoch   9 | 600/957 batches | lr 0.0000 | ms/batch 915.06 | loss 0.0531 | err 0.0128
2025-11-09 00:39:52 - pipeline - INFO - | epoch   9 | 700/957 batches | lr 0.0000 | ms/batch 914.71 | loss 0.0519 | err 0.0125
2025-11-09 00:41:23 - pipeline - INFO - | epoch   9 | 800/957 batches | lr 0.0000 | ms/batch 914.52 | loss 0.0509 | err 0.0122
2025-11-09 00:42:54 - pipeline - INFO - | epoch   9 | 900/957 batches | lr 0.0000 | ms/batch 913.81 | loss 0.0493 | err 0.0119
2025-11-09 00:44:14 - pipeline - INFO - Epoch   9 | train loss 0.0490 | valid loss 0.0679 | valid error 0.0147
2025-11-09 00:44:14 - pipeline - INFO - New best model at epoch 9
2025-11-09 00:45:48 - pipeline - INFO - | epoch  10 | 100/957 batches | lr 0.0000 | ms/batch 934.65 | loss 0.0230 | err 0.0043
2025-11-09 00:47:19 - pipeline - INFO - | epoch  10 | 200/957 batches | lr 0.0000 | ms/batch 916.20 | loss 0.0321 | err 0.0068
2025-11-09 00:48:51 - pipeline - INFO - | epoch  10 | 300/957 batches | lr 0.0000 | ms/batch 915.68 | loss 0.0325 | err 0.0069
2025-11-09 00:50:22 - pipeline - INFO - | epoch  10 | 400/957 batches | lr 0.0000 | ms/batch 915.18 | loss 0.0343 | err 0.0073
2025-11-09 00:51:54 - pipeline - INFO - | epoch  10 | 500/957 batches | lr 0.0000 | ms/batch 915.26 | loss 0.0383 | err 0.0080
2025-11-09 00:53:25 - pipeline - INFO - | epoch  10 | 600/957 batches | lr 0.0000 | ms/batch 914.66 | loss 0.0400 | err 0.0088
2025-11-09 00:54:57 - pipeline - INFO - | epoch  10 | 700/957 batches | lr 0.0000 | ms/batch 914.69 | loss 0.0398 | err 0.0089
2025-11-09 00:56:28 - pipeline - INFO - | epoch  10 | 800/957 batches | lr 0.0000 | ms/batch 915.01 | loss 0.0414 | err 0.0096
2025-11-09 00:58:00 - pipeline - INFO - | epoch  10 | 900/957 batches | lr 0.0000 | ms/batch 914.91 | loss 0.0401 | err 0.0094
2025-11-09 00:59:20 - pipeline - INFO - Epoch  10 | train loss 0.0417 | valid loss 0.0917 | valid error 0.0188
2025-11-09 01:00:53 - pipeline - INFO - | epoch  11 | 100/957 batches | lr 0.0000 | ms/batch 934.26 | loss 0.0264 | err 0.0074
2025-11-09 01:02:25 - pipeline - INFO - | epoch  11 | 200/957 batches | lr 0.0000 | ms/batch 915.24 | loss 0.0321 | err 0.0087
2025-11-09 01:03:56 - pipeline - INFO - | epoch  11 | 300/957 batches | lr 0.0000 | ms/batch 915.30 | loss 0.0361 | err 0.0100
2025-11-09 01:05:28 - pipeline - INFO - | epoch  11 | 400/957 batches | lr 0.0000 | ms/batch 914.95 | loss 0.0329 | err 0.0092
2025-11-09 01:06:59 - pipeline - INFO - | epoch  11 | 500/957 batches | lr 0.0000 | ms/batch 914.65 | loss 0.0330 | err 0.0091
2025-11-09 01:08:31 - pipeline - INFO - | epoch  11 | 600/957 batches | lr 0.0000 | ms/batch 914.50 | loss 0.0381 | err 0.0099
2025-11-09 01:10:02 - pipeline - INFO - | epoch  11 | 700/957 batches | lr 0.0000 | ms/batch 914.88 | loss 0.0383 | err 0.0100
2025-11-09 01:11:34 - pipeline - INFO - | epoch  11 | 800/957 batches | lr 0.0000 | ms/batch 914.58 | loss 0.0378 | err 0.0098
2025-11-09 01:13:05 - pipeline - INFO - | epoch  11 | 900/957 batches | lr 0.0000 | ms/batch 914.07 | loss 0.0383 | err 0.0099
2025-11-09 01:14:25 - pipeline - INFO - Epoch  11 | train loss 0.0371 | valid loss 0.1336 | valid error 0.0235
2025-11-09 01:15:58 - pipeline - INFO - | epoch  12 | 100/957 batches | lr 0.0000 | ms/batch 934.36 | loss 0.0292 | err 0.0068
2025-11-09 01:17:30 - pipeline - INFO - | epoch  12 | 200/957 batches | lr 0.0000 | ms/batch 915.27 | loss 0.0361 | err 0.0093
2025-11-09 01:19:01 - pipeline - INFO - | epoch  12 | 300/957 batches | lr 0.0000 | ms/batch 914.67 | loss 0.0324 | err 0.0087
2025-11-09 01:20:33 - pipeline - INFO - | epoch  12 | 400/957 batches | lr 0.0000 | ms/batch 914.76 | loss 0.0329 | err 0.0087
2025-11-09 01:22:04 - pipeline - INFO - | epoch  12 | 500/957 batches | lr 0.0000 | ms/batch 914.43 | loss 0.0346 | err 0.0086
2025-11-09 01:23:36 - pipeline - INFO - | epoch  12 | 600/957 batches | lr 0.0000 | ms/batch 914.13 | loss 0.0362 | err 0.0093
2025-11-09 01:25:07 - pipeline - INFO - | epoch  12 | 700/957 batches | lr 0.0000 | ms/batch 914.33 | loss 0.0337 | err 0.0086
2025-11-09 01:26:38 - pipeline - INFO - | epoch  12 | 800/957 batches | lr 0.0000 | ms/batch 913.94 | loss 0.0325 | err 0.0082
2025-11-09 01:28:10 - pipeline - INFO - | epoch  12 | 900/957 batches | lr 0.0000 | ms/batch 914.40 | loss 0.0326 | err 0.0080
2025-11-09 01:29:30 - pipeline - INFO - Epoch  12 | train loss 0.0327 | valid loss 0.0790 | valid error 0.0129
2025-11-09 01:31:03 - pipeline - INFO - | epoch  13 | 100/957 batches | lr 0.0000 | ms/batch 933.91 | loss 0.0192 | err 0.0050
2025-11-09 01:32:34 - pipeline - INFO - | epoch  13 | 200/957 batches | lr 0.0000 | ms/batch 915.01 | loss 0.0224 | err 0.0062
2025-11-09 01:34:06 - pipeline - INFO - | epoch  13 | 300/957 batches | lr 0.0000 | ms/batch 915.01 | loss 0.0197 | err 0.0052
2025-11-09 01:35:38 - pipeline - INFO - | epoch  13 | 400/957 batches | lr 0.0000 | ms/batch 915.34 | loss 0.0225 | err 0.0055
2025-11-09 01:37:09 - pipeline - INFO - | epoch  13 | 500/957 batches | lr 0.0000 | ms/batch 915.02 | loss 0.0223 | err 0.0057
2025-11-09 01:38:41 - pipeline - INFO - | epoch  13 | 600/957 batches | lr 0.0000 | ms/batch 914.82 | loss 0.0217 | err 0.0055
2025-11-09 01:40:12 - pipeline - INFO - | epoch  13 | 700/957 batches | lr 0.0000 | ms/batch 914.45 | loss 0.0212 | err 0.0053
2025-11-09 01:41:43 - pipeline - INFO - | epoch  13 | 800/957 batches | lr 0.0000 | ms/batch 914.79 | loss 0.0217 | err 0.0053
2025-11-09 01:43:15 - pipeline - INFO - | epoch  13 | 900/957 batches | lr 0.0000 | ms/batch 915.01 | loss 0.0222 | err 0.0055
2025-11-09 01:44:35 - pipeline - INFO - Epoch  13 | train loss 0.0219 | valid loss 0.0833 | valid error 0.0147
2025-11-09 01:46:08 - pipeline - INFO - | epoch  14 | 100/957 batches | lr 0.0000 | ms/batch 934.13 | loss 0.0147 | err 0.0050
2025-11-09 01:47:40 - pipeline - INFO - | epoch  14 | 200/957 batches | lr 0.0000 | ms/batch 915.30 | loss 0.0201 | err 0.0053
2025-11-09 01:49:11 - pipeline - INFO - | epoch  14 | 300/957 batches | lr 0.0000 | ms/batch 914.93 | loss 0.0190 | err 0.0052
2025-11-09 01:50:43 - pipeline - INFO - | epoch  14 | 400/957 batches | lr 0.0000 | ms/batch 914.47 | loss 0.0192 | err 0.0051
2025-11-09 01:52:14 - pipeline - INFO - | epoch  14 | 500/957 batches | lr 0.0000 | ms/batch 914.80 | loss 0.0197 | err 0.0052
2025-11-09 01:53:46 - pipeline - INFO - | epoch  14 | 600/957 batches | lr 0.0000 | ms/batch 914.48 | loss 0.0198 | err 0.0052
2025-11-09 01:55:17 - pipeline - INFO - | epoch  14 | 700/957 batches | lr 0.0000 | ms/batch 914.43 | loss 0.0195 | err 0.0052
2025-11-09 01:56:48 - pipeline - INFO - | epoch  14 | 800/957 batches | lr 0.0000 | ms/batch 914.00 | loss 0.0177 | err 0.0048
2025-11-09 01:58:20 - pipeline - INFO - | epoch  14 | 900/957 batches | lr 0.0000 | ms/batch 914.01 | loss 0.0178 | err 0.0049
2025-11-09 01:59:40 - pipeline - INFO - Epoch  14 | train loss 0.0174 | valid loss 0.0796 | valid error 0.0135
2025-11-09 02:01:13 - pipeline - INFO - | epoch  15 | 100/957 batches | lr 0.0000 | ms/batch 934.16 | loss 0.0243 | err 0.0080
2025-11-09 02:02:45 - pipeline - INFO - | epoch  15 | 200/957 batches | lr 0.0000 | ms/batch 915.14 | loss 0.0198 | err 0.0059
2025-11-09 02:04:16 - pipeline - INFO - | epoch  15 | 300/957 batches | lr 0.0000 | ms/batch 914.57 | loss 0.0172 | err 0.0052
2025-11-09 02:05:47 - pipeline - INFO - | epoch  15 | 400/957 batches | lr 0.0000 | ms/batch 914.33 | loss 0.0147 | err 0.0045
2025-11-09 02:07:19 - pipeline - INFO - | epoch  15 | 500/957 batches | lr 0.0000 | ms/batch 914.26 | loss 0.0140 | err 0.0042
2025-11-09 02:08:50 - pipeline - INFO - | epoch  15 | 600/957 batches | lr 0.0000 | ms/batch 914.53 | loss 0.0130 | err 0.0040
2025-11-09 02:10:22 - pipeline - INFO - | epoch  15 | 700/957 batches | lr 0.0000 | ms/batch 914.68 | loss 0.0137 | err 0.0038
2025-11-09 02:11:53 - pipeline - INFO - | epoch  15 | 800/957 batches | lr 0.0000 | ms/batch 914.54 | loss 0.0145 | err 0.0040
2025-11-09 02:13:25 - pipeline - INFO - | epoch  15 | 900/957 batches | lr 0.0000 | ms/batch 914.49 | loss 0.0148 | err 0.0041
2025-11-09 02:14:45 - pipeline - INFO - Epoch  15 | train loss 0.0143 | valid loss 0.0770 | valid error 0.0112
2025-11-09 02:16:18 - pipeline - INFO - | epoch  16 | 100/957 batches | lr 0.0000 | ms/batch 934.24 | loss 0.0032 | err 0.0012
2025-11-09 02:17:49 - pipeline - INFO - | epoch  16 | 200/957 batches | lr 0.0000 | ms/batch 915.14 | loss 0.0140 | err 0.0034
2025-11-09 02:19:21 - pipeline - INFO - | epoch  16 | 300/957 batches | lr 0.0000 | ms/batch 914.53 | loss 0.0121 | err 0.0029
2025-11-09 02:20:52 - pipeline - INFO - | epoch  16 | 400/957 batches | lr 0.0000 | ms/batch 914.03 | loss 0.0105 | err 0.0025
2025-11-09 02:22:24 - pipeline - INFO - | epoch  16 | 500/957 batches | lr 0.0000 | ms/batch 913.87 | loss 0.0112 | err 0.0027
2025-11-09 02:23:55 - pipeline - INFO - | epoch  16 | 600/957 batches | lr 0.0000 | ms/batch 913.78 | loss 0.0100 | err 0.0026
2025-11-09 02:25:27 - pipeline - INFO - | epoch  16 | 700/957 batches | lr 0.0000 | ms/batch 914.30 | loss 0.0105 | err 0.0028
2025-11-09 02:26:58 - pipeline - INFO - | epoch  16 | 800/957 batches | lr 0.0000 | ms/batch 914.18 | loss 0.0108 | err 0.0029
2025-11-09 02:28:29 - pipeline - INFO - | epoch  16 | 900/957 batches | lr 0.0000 | ms/batch 914.19 | loss 0.0111 | err 0.0030
2025-11-09 02:29:49 - pipeline - INFO - Epoch  16 | train loss 0.0117 | valid loss 0.0866 | valid error 0.0124
2025-11-09 02:31:23 - pipeline - INFO - | epoch  17 | 100/957 batches | lr 0.0000 | ms/batch 933.60 | loss 0.0067 | err 0.0019
2025-11-09 02:32:54 - pipeline - INFO - | epoch  17 | 200/957 batches | lr 0.0000 | ms/batch 915.16 | loss 0.0111 | err 0.0022
2025-11-09 02:34:26 - pipeline - INFO - | epoch  17 | 300/957 batches | lr 0.0000 | ms/batch 914.56 | loss 0.0102 | err 0.0023
2025-11-09 02:35:57 - pipeline - INFO - | epoch  17 | 400/957 batches | lr 0.0000 | ms/batch 914.14 | loss 0.0094 | err 0.0023
2025-11-09 02:37:28 - pipeline - INFO - | epoch  17 | 500/957 batches | lr 0.0000 | ms/batch 914.16 | loss 0.0077 | err 0.0019
2025-11-09 02:39:00 - pipeline - INFO - | epoch  17 | 600/957 batches | lr 0.0000 | ms/batch 914.36 | loss 0.0084 | err 0.0020
2025-11-09 02:40:31 - pipeline - INFO - | epoch  17 | 700/957 batches | lr 0.0000 | ms/batch 914.31 | loss 0.0077 | err 0.0019
2025-11-09 02:42:03 - pipeline - INFO - | epoch  17 | 800/957 batches | lr 0.0000 | ms/batch 914.36 | loss 0.0074 | err 0.0021
2025-11-09 02:43:34 - pipeline - INFO - | epoch  17 | 900/957 batches | lr 0.0000 | ms/batch 914.42 | loss 0.0080 | err 0.0024
2025-11-09 02:44:54 - pipeline - INFO - Epoch  17 | train loss 0.0084 | valid loss 0.0889 | valid error 0.0135
2025-11-09 02:46:27 - pipeline - INFO - | epoch  18 | 100/957 batches | lr 0.0000 | ms/batch 933.42 | loss 0.0090 | err 0.0019
2025-11-09 02:47:59 - pipeline - INFO - | epoch  18 | 200/957 batches | lr 0.0000 | ms/batch 914.48 | loss 0.0071 | err 0.0019
2025-11-09 02:49:30 - pipeline - INFO - | epoch  18 | 300/957 batches | lr 0.0000 | ms/batch 914.30 | loss 0.0081 | err 0.0023
2025-11-09 02:51:01 - pipeline - INFO - | epoch  18 | 400/957 batches | lr 0.0000 | ms/batch 914.22 | loss 0.0075 | err 0.0022
2025-11-09 02:52:33 - pipeline - INFO - | epoch  18 | 500/957 batches | lr 0.0000 | ms/batch 913.93 | loss 0.0065 | err 0.0019
2025-11-09 02:54:04 - pipeline - INFO - | epoch  18 | 600/957 batches | lr 0.0000 | ms/batch 914.11 | loss 0.0071 | err 0.0020
2025-11-09 02:55:36 - pipeline - INFO - | epoch  18 | 700/957 batches | lr 0.0000 | ms/batch 913.12 | loss 0.0079 | err 0.0022
2025-11-09 02:57:07 - pipeline - INFO - | epoch  18 | 800/957 batches | lr 0.0000 | ms/batch 913.33 | loss 0.0089 | err 0.0023
2025-11-09 02:58:38 - pipeline - INFO - | epoch  18 | 900/957 batches | lr 0.0000 | ms/batch 914.26 | loss 0.0090 | err 0.0025
2025-11-09 02:59:58 - pipeline - INFO - Epoch  18 | train loss 0.0097 | valid loss 0.0834 | valid error 0.0147
2025-11-09 03:01:32 - pipeline - INFO - | epoch  19 | 100/957 batches | lr 0.0000 | ms/batch 934.05 | loss 0.0012 | err 0.0006
2025-11-09 03:03:03 - pipeline - INFO - | epoch  19 | 200/957 batches | lr 0.0000 | ms/batch 915.35 | loss 0.0016 | err 0.0006
2025-11-09 03:04:35 - pipeline - INFO - | epoch  19 | 300/957 batches | lr 0.0000 | ms/batch 914.71 | loss 0.0022 | err 0.0006
2025-11-09 03:06:06 - pipeline - INFO - | epoch  19 | 400/957 batches | lr 0.0000 | ms/batch 914.11 | loss 0.0019 | err 0.0006
2025-11-09 03:07:37 - pipeline - INFO - | epoch  19 | 500/957 batches | lr 0.0000 | ms/batch 913.61 | loss 0.0025 | err 0.0006
2025-11-09 03:09:09 - pipeline - INFO - | epoch  19 | 600/957 batches | lr 0.0000 | ms/batch 913.65 | loss 0.0044 | err 0.0010
2025-11-09 03:10:40 - pipeline - INFO - | epoch  19 | 700/957 batches | lr 0.0000 | ms/batch 913.56 | loss 0.0057 | err 0.0014
2025-11-09 03:12:11 - pipeline - INFO - | epoch  19 | 800/957 batches | lr 0.0000 | ms/batch 913.35 | loss 0.0058 | err 0.0015
2025-11-09 03:13:43 - pipeline - INFO - | epoch  19 | 900/957 batches | lr 0.0000 | ms/batch 913.26 | loss 0.0058 | err 0.0015
2025-11-09 03:15:02 - pipeline - INFO - Epoch  19 | train loss 0.0062 | valid loss 0.0778 | valid error 0.0112
2025-11-09 03:16:36 - pipeline - INFO - | epoch  20 | 100/957 batches | lr 0.0000 | ms/batch 932.53 | loss 0.0048 | err 0.0019
2025-11-09 03:18:07 - pipeline - INFO - | epoch  20 | 200/957 batches | lr 0.0000 | ms/batch 913.74 | loss 0.0039 | err 0.0012
2025-11-09 03:19:38 - pipeline - INFO - | epoch  20 | 300/957 batches | lr 0.0000 | ms/batch 913.72 | loss 0.0030 | err 0.0010
2025-11-09 03:21:10 - pipeline - INFO - | epoch  20 | 400/957 batches | lr 0.0000 | ms/batch 913.55 | loss 0.0023 | err 0.0008
2025-11-09 03:22:41 - pipeline - INFO - | epoch  20 | 500/957 batches | lr 0.0000 | ms/batch 913.42 | loss 0.0031 | err 0.0010
2025-11-09 03:24:13 - pipeline - INFO - | epoch  20 | 600/957 batches | lr 0.0000 | ms/batch 913.43 | loss 0.0026 | err 0.0008
2025-11-09 03:25:44 - pipeline - INFO - | epoch  20 | 700/957 batches | lr 0.0000 | ms/batch 913.60 | loss 0.0032 | err 0.0009
2025-11-09 03:27:15 - pipeline - INFO - | epoch  20 | 800/957 batches | lr 0.0000 | ms/batch 913.78 | loss 0.0043 | err 0.0011
2025-11-09 03:28:47 - pipeline - INFO - | epoch  20 | 900/957 batches | lr 0.0000 | ms/batch 913.86 | loss 0.0050 | err 0.0012
2025-11-09 03:30:06 - pipeline - INFO - Epoch  20 | train loss 0.0053 | valid loss 0.0794 | valid error 0.0112
2025-11-09 03:31:40 - pipeline - INFO - | epoch  21 | 100/957 batches | lr 0.0000 | ms/batch 932.57 | loss 0.0045 | err 0.0006
2025-11-09 03:33:11 - pipeline - INFO - | epoch  21 | 200/957 batches | lr 0.0000 | ms/batch 913.80 | loss 0.0068 | err 0.0019
2025-11-09 03:34:42 - pipeline - INFO - | epoch  21 | 300/957 batches | lr 0.0000 | ms/batch 913.90 | loss 0.0053 | err 0.0017
2025-11-09 03:36:14 - pipeline - INFO - | epoch  21 | 400/957 batches | lr 0.0000 | ms/batch 913.58 | loss 0.0046 | err 0.0014
2025-11-09 03:37:45 - pipeline - INFO - | epoch  21 | 500/957 batches | lr 0.0000 | ms/batch 913.26 | loss 0.0039 | err 0.0011
2025-11-09 03:39:16 - pipeline - INFO - | epoch  21 | 600/957 batches | lr 0.0000 | ms/batch 913.37 | loss 0.0040 | err 0.0012
2025-11-09 03:40:48 - pipeline - INFO - | epoch  21 | 700/957 batches | lr 0.0000 | ms/batch 913.24 | loss 0.0043 | err 0.0013
2025-11-09 03:42:19 - pipeline - INFO - | epoch  21 | 800/957 batches | lr 0.0000 | ms/batch 912.94 | loss 0.0042 | err 0.0012
2025-11-09 03:43:50 - pipeline - INFO - | epoch  21 | 900/957 batches | lr 0.0000 | ms/batch 913.12 | loss 0.0039 | err 0.0012
2025-11-09 03:45:10 - pipeline - INFO - Epoch  21 | train loss 0.0037 | valid loss 0.0916 | valid error 0.0124
2025-11-09 03:46:43 - pipeline - INFO - | epoch  22 | 100/957 batches | lr 0.0000 | ms/batch 932.71 | loss 0.0002 | err 0.0000
2025-11-09 03:48:15 - pipeline - INFO - | epoch  22 | 200/957 batches | lr 0.0000 | ms/batch 913.86 | loss 0.0015 | err 0.0003
2025-11-09 03:49:46 - pipeline - INFO - | epoch  22 | 300/957 batches | lr 0.0000 | ms/batch 914.25 | loss 0.0016 | err 0.0004
2025-11-09 03:51:18 - pipeline - INFO - | epoch  22 | 400/957 batches | lr 0.0000 | ms/batch 914.40 | loss 0.0017 | err 0.0005
2025-11-09 03:52:49 - pipeline - INFO - | epoch  22 | 500/957 batches | lr 0.0000 | ms/batch 913.42 | loss 0.0020 | err 0.0005
2025-11-09 03:54:20 - pipeline - INFO - | epoch  22 | 600/957 batches | lr 0.0000 | ms/batch 913.53 | loss 0.0021 | err 0.0005
2025-11-09 03:55:52 - pipeline - INFO - | epoch  22 | 700/957 batches | lr 0.0000 | ms/batch 913.46 | loss 0.0025 | err 0.0007
2025-11-09 03:57:23 - pipeline - INFO - | epoch  22 | 800/957 batches | lr 0.0000 | ms/batch 913.39 | loss 0.0025 | err 0.0007
2025-11-09 03:58:54 - pipeline - INFO - | epoch  22 | 900/957 batches | lr 0.0000 | ms/batch 913.57 | loss 0.0030 | err 0.0009
2025-11-09 04:00:14 - pipeline - INFO - Epoch  22 | train loss 0.0028 | valid loss 0.0895 | valid error 0.0129
2025-11-09 04:01:47 - pipeline - INFO - | epoch  23 | 100/957 batches | lr 0.0000 | ms/batch 933.83 | loss 0.0016 | err 0.0006
2025-11-09 04:03:19 - pipeline - INFO - | epoch  23 | 200/957 batches | lr 0.0000 | ms/batch 914.42 | loss 0.0014 | err 0.0006
2025-11-09 04:04:50 - pipeline - INFO - | epoch  23 | 300/957 batches | lr 0.0000 | ms/batch 913.93 | loss 0.0013 | err 0.0006
2025-11-09 04:06:22 - pipeline - INFO - | epoch  23 | 400/957 batches | lr 0.0000 | ms/batch 913.99 | loss 0.0014 | err 0.0006
2025-11-09 04:07:53 - pipeline - INFO - | epoch  23 | 500/957 batches | lr 0.0000 | ms/batch 913.83 | loss 0.0012 | err 0.0005
2025-11-09 04:09:24 - pipeline - INFO - | epoch  23 | 600/957 batches | lr 0.0000 | ms/batch 913.90 | loss 0.0011 | err 0.0005
2025-11-09 04:10:56 - pipeline - INFO - | epoch  23 | 700/957 batches | lr 0.0000 | ms/batch 914.05 | loss 0.0010 | err 0.0004
2025-11-09 04:12:27 - pipeline - INFO - | epoch  23 | 800/957 batches | lr 0.0000 | ms/batch 913.83 | loss 0.0009 | err 0.0004
2025-11-09 04:13:59 - pipeline - INFO - | epoch  23 | 900/957 batches | lr 0.0000 | ms/batch 913.66 | loss 0.0015 | err 0.0005
2025-11-09 04:15:18 - pipeline - INFO - Epoch  23 | train loss 0.0020 | valid loss 0.0957 | valid error 0.0135
2025-11-09 04:16:52 - pipeline - INFO - | epoch  24 | 100/957 batches | lr 0.0000 | ms/batch 933.40 | loss 0.0038 | err 0.0012
2025-11-09 04:18:23 - pipeline - INFO - | epoch  24 | 200/957 batches | lr 0.0000 | ms/batch 914.26 | loss 0.0021 | err 0.0006
2025-11-09 04:19:55 - pipeline - INFO - | epoch  24 | 300/957 batches | lr 0.0000 | ms/batch 914.00 | loss 0.0015 | err 0.0004
2025-11-09 04:21:26 - pipeline - INFO - | epoch  24 | 400/957 batches | lr 0.0000 | ms/batch 913.73 | loss 0.0011 | err 0.0003
2025-11-09 04:22:57 - pipeline - INFO - | epoch  24 | 500/957 batches | lr 0.0000 | ms/batch 913.97 | loss 0.0011 | err 0.0004
2025-11-09 04:24:29 - pipeline - INFO - | epoch  24 | 600/957 batches | lr 0.0000 | ms/batch 913.87 | loss 0.0011 | err 0.0005
2025-11-09 04:26:00 - pipeline - INFO - | epoch  24 | 700/957 batches | lr 0.0000 | ms/batch 913.80 | loss 0.0010 | err 0.0004
2025-11-09 04:27:31 - pipeline - INFO - | epoch  24 | 800/957 batches | lr 0.0000 | ms/batch 913.71 | loss 0.0013 | err 0.0006
2025-11-09 04:29:03 - pipeline - INFO - | epoch  24 | 900/957 batches | lr 0.0000 | ms/batch 913.63 | loss 0.0012 | err 0.0006
2025-11-09 04:30:23 - pipeline - INFO - Epoch  24 | train loss 0.0012 | valid loss 0.1089 | valid error 0.0159
2025-11-09 04:31:56 - pipeline - INFO - | epoch  25 | 100/957 batches | lr 0.0000 | ms/batch 932.99 | loss 0.0001 | err 0.0000
2025-11-09 04:33:27 - pipeline - INFO - | epoch  25 | 200/957 batches | lr 0.0000 | ms/batch 913.38 | loss 0.0003 | err 0.0003
2025-11-09 04:34:59 - pipeline - INFO - | epoch  25 | 300/957 batches | lr 0.0000 | ms/batch 913.51 | loss 0.0025 | err 0.0008
2025-11-09 04:36:30 - pipeline - INFO - | epoch  25 | 400/957 batches | lr 0.0000 | ms/batch 913.44 | loss 0.0019 | err 0.0006
2025-11-09 04:38:01 - pipeline - INFO - | epoch  25 | 500/957 batches | lr 0.0000 | ms/batch 913.47 | loss 0.0015 | err 0.0005
2025-11-09 04:39:33 - pipeline - INFO - | epoch  25 | 600/957 batches | lr 0.0000 | ms/batch 913.79 | loss 0.0017 | err 0.0005
2025-11-09 04:41:04 - pipeline - INFO - | epoch  25 | 700/957 batches | lr 0.0000 | ms/batch 913.54 | loss 0.0014 | err 0.0004
2025-11-09 04:42:35 - pipeline - INFO - | epoch  25 | 800/957 batches | lr 0.0000 | ms/batch 913.43 | loss 0.0013 | err 0.0004
2025-11-09 04:44:07 - pipeline - INFO - | epoch  25 | 900/957 batches | lr 0.0000 | ms/batch 913.32 | loss 0.0012 | err 0.0003
2025-11-09 04:45:26 - pipeline - INFO - Epoch  25 | train loss 0.0012 | valid loss 0.1022 | valid error 0.0141
2025-11-09 04:45:26 - pipeline - INFO - Training complete
2025-11-09 04:45:29 - pipeline - INFO - Saved model weights to results/mouse_opossum/scgpt_Nov08-22-26/model_outputs/best_model.pt
2025-11-09 04:45:29 - pipeline - INFO - Saved vocabulary to results/mouse_opossum/scgpt_Nov08-22-26/model_outputs/vocab.json
2025-11-09 04:45:29 - pipeline - INFO - 
=========================================================================================
2025-11-09 04:45:29 - pipeline - INFO - STEP 6: Evaluating Model
2025-11-09 04:45:29 - pipeline - INFO - =========================================================================================
2025-11-09 04:45:29 - pipeline - INFO - -----------------------------------------------------------------------------------------
2025-11-09 04:45:29 - pipeline - INFO - Evaluating on opossum
2025-11-09 04:45:29 - pipeline - INFO - -----------------------------------------------------------------------------------------
2025-11-09 04:45:29 - pipeline - INFO - Predicting labels for query data
2025-11-09 04:45:29 - pipeline - INFO - Converting gene names to uppercase to match vocabulary
2025-11-09 04:45:29 - pipeline - INFO - Matched 11204/11730 genes in vocabulary of size 60697
scGPT - INFO - Filtering cells by counts ...
scGPT - INFO - Normalizing total counts ...
scGPT - INFO - Binning data ...
2025-11-09 04:45:50 - pipeline - INFO - Using 11204 common genes for prediction
2025-11-09 04:54:55 - pipeline - INFO - Generated predictions for 32764 cells
2025-11-09 04:54:55 - pipeline - INFO - Results (18305 cells with valid labels):
2025-11-09 04:54:55 - pipeline - INFO -   Accuracy: 0.7577
2025-11-09 04:54:55 - pipeline - INFO -   Precision (macro): 0.5016
2025-11-09 04:54:55 - pipeline - INFO -   Recall (macro): 0.4820
2025-11-09 04:54:55 - pipeline - INFO -   F1 (macro): 0.4725
2025-11-09 04:54:56 - pipeline - INFO - Predictions saved to results/mouse_opossum/scgpt_Nov08-22-26/predictions_opossum.csv
2025-11-09 04:54:56 - pipeline - INFO - Metrics saved to results/mouse_opossum/scgpt_Nov08-22-26/metrics_opossum.json
2025-11-09 04:54:56 - pipeline - INFO - Generating visualizations...
====== Confusion Matrix ======
           L2/3        L4      L5IT  ...       OPC      Endo      VLMC
IT_A   0.625431  0.063882  0.006997  ...  0.000101  0.002028  0.000507
IT_B   0.251448  0.502317  0.050985  ...  0.001738  0.000000  0.000579
IT_C   0.022018  0.082866  0.425540  ...  0.000000  0.000000  0.000000
L5NP   0.001355  0.021680  0.536585  ...  0.005420  0.000000  0.000000
IT_D   0.284182  0.077748  0.000000  ...  0.000000  0.037534  0.000000
L5PT   0.021559  0.005804  0.024876  ...  0.000000  0.000000  0.000000
L6CT   0.000000  0.001374  0.058549  ...  0.003299  0.000825  0.006872
L6b    0.000000  0.000000  0.036697  ...  0.000000  0.000000  0.000000
Pvalb  0.000000  0.000000  0.000000  ...  0.000933  0.000000  0.000000
Sst    0.000978  0.000000  0.003914  ...  0.000000  0.000000  0.000000
Vip    0.000000  0.009881  0.001976  ...  0.005929  0.001976  0.005929
Lamp5  0.000000  0.000000  0.000000  ...  0.002703  0.000000  0.000000
Frem1  0.000000  0.006557  0.000000  ...  0.009836  0.000000  0.003279
Astro  0.000000  0.000000  0.000000  ...  0.000000  0.002393  0.002872
Micro  0.000000  0.000000  0.000000  ...  0.000000  0.001383  0.165975
OD     0.000000  0.000000  0.000000  ...  0.000247  0.000000  0.000000
OPC    0.000000  0.000745  0.000000  ...  0.670886  0.000000  0.001489
Endo   0.000000  0.000000  0.000000  ...  0.000000  0.938462  0.046154

[18 rows x 20 columns]
==============================
2025-11-09 04:54:57 - pipeline - INFO - Confusion matrix saved to results/mouse_opossum/scgpt_Nov08-22-26/confusion_matrix_opossum.png
/u/scratch/r/rgorzek/foundation-cross/src/analysis/visualization.py:245: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df, x='F1', y='Class', palette='viridis')
2025-11-09 04:54:58 - pipeline - INFO - Per-class F1 plot saved to results/mouse_opossum/scgpt_Nov08-22-26/per_class_f1_opossum.png
2025-11-09 04:54:58 - pipeline - INFO - 
=========================================================================================
2025-11-09 04:54:58 - pipeline - INFO - PIPELINE COMPLETE
2025-11-09 04:54:58 - pipeline - INFO - =========================================================================================
2025-11-09 04:54:58 - pipeline - INFO - Total time: 23327.41 seconds

Experiment complete! Results saved to: results/mouse_opossum/scgpt_Nov08-22-26
==========================================
Job 11346973 ended on:    g14
Job 11346973 ended on:    Sun Nov 9 04:55:00 PST 2025
 
==========================================
